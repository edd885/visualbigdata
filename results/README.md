### **Resultados Esperados de los Challenges**

Los tres desafíos planteados en esta asignatura tienen como objetivo desarrollar habilidades técnicas y analíticas esenciales para el manejo de grandes bases de datos. Al completar cada uno de los challenges, se espera que los estudiantes adquieran los siguientes resultados de aprendizaje y competencias técnicas:

---

### **Challenge 1: Introducción a PySpark y Koalas**
- **Resultados Técnicos Esperados**:
  - Familiarización con la instalación y configuración básica de **PySpark** en entornos como Databricks.
  - Capacidad para importar y manipular bases de datos masivas utilizando la API de **Koalas**, aprovechando la similitud con pandas para facilitar la transición a entornos distribuidos.
  - Competencia en el uso de herramientas de visualización básicas, como **Matplotlib** y **Plotly**, en el contexto de análisis de datos masivos.
  - Comprensión del flujo de trabajo para cargar, transformar y explorar grandes conjuntos de datos en un entorno distribuido.
  - Mejora en la organización y limpieza del código, promoviendo prácticas de trabajo reproducibles y colaborativas en GitHub.

- **Impacto en el Alumno**:
  Este challenge provee una base sólida para abordar problemas de análisis de datos en Big Data, permitiendo al alumno reconocer las diferencias clave entre herramientas tradicionales y distribuidas. Además, fomenta la confianza en el manejo inicial de PySpark.

---

### **Challenge 2: Modelado con Spark ML**
- **Resultados Técnicos Esperados**:
  - Desarrollo de habilidades para preprocesar datos masivos, incluyendo la limpieza, transformación y partición de conjuntos de datos.
  - Comprensión del pipeline de **machine learning** en Spark ML, incluyendo la selección y entrenamiento de modelos de clasificación (como la regresión logística).
  - Capacidad para evaluar la calidad del modelo mediante métricas estándar (ej., precisión, recall) y optimizar hiperparámetros para mejorar el desempeño.
  - Documentación adecuada de los resultados obtenidos, reforzando la importancia de la reproducibilidad en proyectos de ciencia de datos.
  - Integración de herramientas distribuidas para resolver problemas reales de clasificación, superando las limitaciones de procesamiento en sistemas locales.

- **Impacto en el Alumno**:
  Los estudiantes desarrollan competencias avanzadas en machine learning aplicado a Big Data, permitiéndoles implementar modelos predictivos en problemas prácticos. Este conocimiento es directamente transferible a escenarios del mundo real donde el volumen de datos es desafiante.

---

### **Challenge 3: Análisis Avanzado con PySpark**
- **Resultados Técnicos Esperados**:.
  - Competencia en el desarrollo de modelos predictivos avanzados con **Spark ML**, aplicando técnicas como clustering, clasificación avanzada o series temporales. Dado que el proyecto personal es SMART DATA, el uso de pyspark se ve usado no en gran medida, sin embargo se aprendió mucho para futuros proyectos.
  - Integración de datos relevantes a la investigación de tesis o proyectos personales, demostrando la aplicabilidad de PySpark en contextos diversos.
  - Dominio en la presentación de resultados complejos, incluyendo la creación de visualizaciones e informes claros y estructurados.
  - Organización profesional del código y documentación en GitHub, consolidando habilidades en control de versiones y trabajo colaborativo.

- **Impacto en el Alumno**:
  Este desafío impulsa a los estudiantes a aplicar PySpark en problemas específicos y complejos relacionados con sus intereses académicos o profesionales. La capacidad de trabajar con datos en tiempo real y construir modelos avanzados posiciona a los alumnos como profesionales preparados para enfrentar retos en la industria y la academia.

---

### **Impacto General en los Estudiantes**
Al finalizar los tres challenges, los estudiantes habrán adquirido competencias fundamentales para manejar, analizar y visualizar grandes volúmenes de datos en un entorno distribuido. Entre los aprendizajes más destacados se incluyen:

1. **Habilidades técnicas esenciales**:
   - Configuración y uso de entornos distribuidos como PySpark y Databricks.
   - Aplicación práctica de machine learning en problemas reales.
   - Visualización eficiente de datos masivos.

2. **Competencias analíticas y estratégicas**:
   - Capacidad para seleccionar herramientas adecuadas según el contexto del problema.
   - Uso de APIs avanzadas para resolver problemas específicos.

3. **Preparación para proyectos reales**:
   - Uso de bases de datos públicas como Kaggle o UCI Machine Learning Repository.
   - Trabajo reproducible y profesional documentado en GitHub.

4. **Transferencia de conocimientos**:
   - Relación directa entre lo aprendido y temas de tesis o proyectos personales.
   - Confianza para aplicar técnicas avanzadas en escenarios reales.

Estos resultados posicionan a los estudiantes como profesionales altamente competentes en el uso de tecnologías avanzadas para la visualización y análisis de grandes bases de datos, habilidades cruciales en la ciencia de datos moderna.

---
